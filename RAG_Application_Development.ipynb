{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPVCDXvYyqIg"
   },
   "source": [
    "### Project: Multilingual RAG Model for Finnish and English Language Support\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Description:\n",
    "\n",
    "**This project involves the development of a sophisticated Retrieval-Augmented Generation (RAG) model capable of providing accurate, contextually relevant responses in both Finnish and English. The model leverages state-of-the-art natural language processing (NLP) techniques, integrating retrieval mechanisms to augment the generative process, ensuring that the answers are not only relevant but also linguistically appropriate in both languages. The goal is to create a seamless experience for users interacting in either language, allowing for efficient cross-lingual capabilities in real-time applications**.\n",
    "\n",
    "**The solution employs custom data pipelines, document loaders, and language-specific models for both Finnish and English, ensuring high-quality results across diverse use cases. By utilizing advanced vector databases and context-aware retrieval, the system generates dynamic responses tailored to the user’s input, regardless of language**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D4MjtzU0AON"
   },
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lq0xsyemPle_",
    "outputId": "f559bbf7-38aa-47fc-dcea-99a76fd66a11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install langchain\\n!pip install -U langchain-community\\n!pip install -qU pypdf\\n!pip install -qU langchain-cohere\\n!pip install langid\\n!pip install -U deep-translator'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU langchain-cohere\n",
    "!pip install langid\n",
    "!pip install -U deep-translator\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6WnNOih0IL7"
   },
   "source": [
    "### Import Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pxXvaAsrg7Zi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader # Importing PyPDFLoader to handle loading PDF files as documents.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing RecursiveCharacterTextSplitter to split text content into smaller chunks recursively, based on character count.\n",
    "from langchain_cohere import ChatCohere # Importing Cohere model for language modeling tasks, allowing the use of Cohere’s language generation capabilities.\n",
    "from langchain_cohere import CohereEmbeddings # Importing CohereEmbeddings for creating embeddings using Cohere, which can be useful for similarity searches or semantic understanding.\n",
    "from langchain_chroma import Chroma # Importing Chroma vector store from Langchain to store and retrieve embeddings, enabling vector-based similarity searches.\n",
    "import os # Importing os module for operating system functionalities.\n",
    "import langid # Importing langid for language detection functionalities.\n",
    "from langchain.prompts import PromptTemplate # Using Prompt to Provide Context and Query.\n",
    "from langchain_core.runnables import RunnableSequence  # Importing Sequence to run llmchains\n",
    "from deep_translator import GoogleTranslator # Language translate\n",
    "from dotenv import load_dotenv # Loading Env varaibles.\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxjgkX8caB43"
   },
   "source": [
    "### Implementation: Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMbc-bcN0Y_l"
   },
   "source": [
    "## Loading in the Data\n",
    "---\n",
    "#### 1. The First Step Is to load Documents I am using Langchain community to load documents ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geBcejdxwsuN",
    "outputId": "a1c13c57-ccd9-440e-ca87-4511bf489328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Successfully Added in pdf_data dictionary Thank you :)\n",
      "There are Total 2 PDF files present in the directory at the moment\n"
     ]
    }
   ],
   "source": [
    "# Function to load PDF files from a directory and extract pages asynchronously\n",
    "async def load_pdfs_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads all PDF files in the given directory and extracts their pages asynchronously.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are file names and values are the list of pages extracted from each PDF.\n",
    "    \"\"\"\n",
    "    # Dictionary to store pages extracted from each PDF file\n",
    "    pdf_pages = {}\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        # Check if the file is a PDF (you can adjust this based on your file extensions)\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            loader = PyPDFLoader(file_path=file_path)\n",
    "            pages = []\n",
    "\n",
    "            # Asynchronously load pages from the PDF file\n",
    "            async for page in loader.alazy_load():\n",
    "                pages.append(page)\n",
    "\n",
    "            # Store the extracted pages in the dictionary, using the file name as the key\n",
    "            pdf_pages[file_name] = pages\n",
    "\n",
    "    # Return the dictionary containing pages for each file\n",
    "    return pdf_pages\n",
    "\n",
    "# Input Directory path\n",
    "directory_path = input(\"Please enter the path to the directory containing your PDF files: \")\n",
    "pdf_data = await load_pdfs_from_directory(directory_path)\n",
    "print(\"Documents Successfully Added in pdf_data dictionary Thank you :)\")\n",
    "print(f'There are Total {len(pdf_data.keys())} PDF files present in the directory at the moment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4AmawrK2jmE"
   },
   "source": [
    "## Chunking\n",
    "---\n",
    "#### 2. The Second Step Is to Convert the Data Into Chunks Due to Token Limit ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctsoksxg5ZES",
    "outputId": "828287a6-e27c-442a-b290-aeead6826ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total split documents: 528\n"
     ]
    }
   ],
   "source": [
    "# In this Function to split documents into chunks (size, overlap)\n",
    "def split_documents(pdf_data, chunk_size=500, chunk_overlap=25):\n",
    "    \"\"\"\n",
    "    Splits the extracted PDF pages into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        pdf_data (dict): Dictionary containing PDF file names as keys and list of Document objects (each representing a page) as values.\n",
    "        chunk_size (int): The size of each chunk of text.\n",
    "        chunk_overlap (int): The overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are file names and values are lists of split document chunks.\n",
    "    \"\"\"\n",
    "    # Initialize the text splitter\n",
    "    # Size of each chunk in characters, # Overlap between consecutive chunks, # Function to compute the length of the text,\n",
    "    # Flag to add start index to each chunk.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap,length_function=len,add_start_index=True)\n",
    "\n",
    "    # Dictionary to store the split documents\n",
    "    split_docs = {}\n",
    "\n",
    "    # Iterate over each PDF file's data\n",
    "    for file_name, documents in pdf_data.items():\n",
    "        # Split the documents (pages) into chunks\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Store the chunks in the dictionary\n",
    "        split_docs[file_name] = chunks\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "# Now split the documents into chunks\n",
    "split_pdf_data = split_documents(pdf_data)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total split documents: {sum([len(chunks) for chunks in split_pdf_data.values()])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnUgliK-2VlZ"
   },
   "source": [
    "## Combining Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X10XHztOWVIl"
   },
   "outputs": [],
   "source": [
    "# In this Function, I am combining all the PDF's under one list so I can embed and store into a vector database.\n",
    "def combine_documents(split_pdf_data):\n",
    "    \"\"\"\n",
    "    Combines all the PDF documents stored in a dictionary into a single list.\n",
    "\n",
    "    Args:\n",
    "        split_pdf_data (dict): A dictionary where the values are lists of text chunks or pages from the PDFs.\n",
    "\n",
    "    Returns:\n",
    "        list: A single list containing all the text chunks/pages from all PDFs.\n",
    "    \"\"\"\n",
    "    return sum(split_pdf_data.values(), [])\n",
    "\n",
    "# Assuming split_pdf_data is a dictionary where the values are lists of text chunks or pages\n",
    "combine_all_documents = combine_documents(split_pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1UBxUUqBf7W"
   },
   "source": [
    "## Embeddings and Data Storage\n",
    "---\n",
    "#### 3 . The Third Step Is To Embedded The Sentences and store the data into Database. For that purpose i am Using Chroma Because Its Supports Mutlilanguage Modelling ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_DXFRhaNW7O",
    "outputId": "95e2dfe7-7c75-4bdc-b4e6-429da81a299d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents successfully stored in Chroma DB :)\n"
     ]
    }
   ],
   "source": [
    "# This Function Apply Embeddings and Storing Data into Chroma DB Vector Database.\n",
    "def store_documents_in_chroma(documents, api_key, persist_directory=\"./chroma_storage\"):\n",
    "    \"\"\"\n",
    "    Stores documents in a Chroma database using multilingual embeddings from Cohere.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents to be embedded and stored in the Chroma database.\n",
    "        api_key (str): API key for accessing Cohere's embedding service.\n",
    "        collection_name (str, optional): Name of the collection in the Chroma database. Defaults to \"multilingual_documents\".\n",
    "        persist_directory (str, optional): Directory path to save the Chroma database. Defaults to \"./chroma_storage\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up environment variable for Cohere API key\n",
    "    os.environ[\"COHERE_API_KEY\"] = api_key\n",
    "\n",
    "    # Initialize the Cohere embedding model with multilingual capabilities\n",
    "    embedding_model = CohereEmbeddings(model=\"embed-multilingual-v3.0\")\n",
    "\n",
    "    # Initialize Chroma vector store with specified collection and persistence settings\n",
    "    chroma_db = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "\n",
    "    # Saving Into Chroma DB Permanently\n",
    "    #chroma_db.persist()\n",
    "\n",
    "    # Output confirmation message\n",
    "    print(\"Documents successfully stored in Chroma DB :)\")\n",
    "\n",
    "# Example usage\n",
    "api_key = os.environ.get('COHERE_API_KEY')\n",
    "store_documents_in_chroma(combine_all_documents, api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ICdBPy0f7iy"
   },
   "source": [
    "## Retrival Using Vector Similarity Search\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Here I am using Vector-similarity search which computes a distance metric between the query vectors and indexed vectors in the database. This method is more effective typically for retrieving contextually relevant information to the prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fruYwbpAIpxv"
   },
   "outputs": [],
   "source": [
    "# This Function loads the Chroma DB and queries the relevant documents based on the query text.\n",
    "def load_and_query_chroma(persist_directory=\"./chroma_storage\", query_text=\"none\", k=3):\n",
    "    \"\"\"\n",
    "    Loads a persisted Chroma database and runs a similarity search on it.\n",
    "    Automatically detects the language of the query and provides results in the same language.\n",
    "\n",
    "    Args:\n",
    "        persist_directory (str): Directory where the Chroma database is stored.\n",
    "        query_text (str): The input query for similarity search.\n",
    "        k (int): Number of nearest neighbors to retrieve from the search.\n",
    "\n",
    "    Returns:\n",
    "        results (str): List of search results with relevance scores or a message indicating no suitable match.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Cohere multilingual embedding model\n",
    "    embedding_model = CohereEmbeddings(model=\"embed-multilingual-v3.0\")\n",
    "\n",
    "    # Load the Chroma DB from the persistence directory\n",
    "    chroma_db = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "\n",
    "    try:\n",
    "        # Detect the language of the query using langid\n",
    "        query_language, _ = langid.classify(query_text)\n",
    "\n",
    "        # Check if the language is supported (English or Finnish)\n",
    "        if query_language not in [\"en\", \"fi\"]:\n",
    "            return \"This system currently supports queries in English and Finnish only.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"Error detecting language. This system currently supports queries in English and Finnish only.\"\n",
    "\n",
    "    # Perform similarity search\n",
    "    results = chroma_db.similarity_search_with_relevance_scores(query=query_text, k=k)\n",
    "\n",
    "    # Check if any results meet the relevance threshold\n",
    "    if not results or results[0][1] < 0.3:\n",
    "        if query_language == \"fi\":\n",
    "            return \"Emme pystyneet löytämään sopivaa vastausta tähän kyselyyn.\"\n",
    "        else:  # Default to English for non-Finnish detected languages\n",
    "            return \"We are unable to find a suitable match for this query.\"\n",
    "\n",
    "    # Extract page content from each result\n",
    "    formatted_results = [result.page_content for result, score in results]\n",
    "\n",
    "    # Join results into a single string\n",
    "    joined_results = \" \".join(formatted_results)\n",
    "\n",
    "    # Detect the language of the results (if they are different from the query language, translate them)\n",
    "    result_language, _ = langid.classify(joined_results)\n",
    "\n",
    "    if query_language != result_language:\n",
    "        # Translate results to the language of the query\n",
    "        translated_results = GoogleTranslator(source=result_language, target=query_language).translate(joined_results)\n",
    "        return translated_results\n",
    "    else:\n",
    "        # Return results in their original language if no translation is needed\n",
    "        return joined_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inwlVFv9inj6"
   },
   "source": [
    "## Combining Context and Query\n",
    "\n",
    "---\n",
    "#### 5. I am using ChatCohere model in order to generate Response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQU2gVesUHVu"
   },
   "outputs": [],
   "source": [
    "# Function to create a formatted prompt using specific context and question input\n",
    "def create_formatted_prompt(context, question):\n",
    "    \"\"\"\n",
    "    Generates a formatted prompt for answering questions in a structured and professional manner.\n",
    "\n",
    "    Parameters:\n",
    "    - context (str): The context or background information to base the answer on.\n",
    "    - question (str): The question to be answered.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted prompt with clear guidelines for the response format and language.\n",
    "    \"\"\"\n",
    "\n",
    "    # Enhanced prompt template to ensure precise, relevant, and well-structured responses\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a professional assistant with expertise in the subject matter. Your goal is to provide the most accurate, clear, and relevant response based on the context provided. Ensure the response is well-organized, insightful, and demonstrates a deep understanding of the topic.\n",
    "    Response should be in what language question was asked.\n",
    "        \n",
    "    Context:\n",
    "    {context}\n",
    "        \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the PromptTemplate with defined input variables for context and question\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=PROMPT_TEMPLATE\n",
    "    )\n",
    "\n",
    "    # Format the template with the given context and question, returning a final prompt\n",
    "    return prompt_template.format(context=context, question=question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "eOQM1y_ThK-F",
    "outputId": "783bc821-837a-4bf7-a9c5-67ffb3f0e4e4"
   },
   "outputs": [],
   "source": [
    "# Main function to generate response\n",
    "def generate_response(query):\n",
    "    \"\"\"\n",
    "    Generates a response to a query by retrieving context from the Chroma database and\n",
    "    generating an answer using a Cohere LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user's question or input query.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated response based on the context and question.\n",
    "    \"\"\"\n",
    "    # Assuming load_and_query_chroma is a valid function that fetches context\n",
    "    context = load_and_query_chroma(query_text=query)\n",
    "\n",
    "    # Generate the formatted prompt\n",
    "    if context == \"Emme pystyneet löytämään sopivaa vastausta tähän kyselyyn.\":\n",
    "        return \"Emme valitettavasti löytäneet sopivaa vastausta tähän kyselyyn. Voisitko ystävällisesti tarkentaa tai muotoilla kysymyksesi uudelleen, jotta voimme auttaa sinua paremmin?\"  # Return the Finnish no-answer message\n",
    "    elif context == \"We are unable to find a suitable match for this query.\":\n",
    "        return \"Unfortunately, we were unable to find a suitable answer to this query. Could you kindly clarify or rephrase your question so that we can assist you better?\"  # Return the English no-answer message\n",
    "    \n",
    "    prompt = create_formatted_prompt(context=context, question=query)\n",
    "\n",
    "    # Initialize Cohere LLM (assuming environment variable is set for the API key)\n",
    "    llm = ChatCohere()\n",
    "\n",
    "    # Create a prompt template for the LLMChain\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=prompt\n",
    "    )\n",
    "\n",
    "    # Create the LLMChain with the prompt template and LLM\n",
    "    #llm_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "    llm_chain = RunnableSequence(prompt_template, llm)\n",
    "\n",
    "\n",
    "    # Run the chain to generate an answer\n",
    "    generated_answer = llm_chain.invoke({\"context\": context, \"question\": query})\n",
    "    return generated_answer.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Uninterrupted Two-Shift Work:**\\n- The context mentions that the transition to uninterrupted two-shift work will be agreed upon locally, suggesting that the specific terms may vary depending on the location or the agreement between the parties involved.\\n- It is stated that this arrangement will not come into effect before 01.12.2021, indicating a specific start date for the implementation of the two-shift system.\\n- The company will have uninterrupted access to working time systems related to two-shift work, implying that there are established systems or regulations in place to manage this type of work schedule.\\n\\n**Uninterrupted Three-Shift Work:**\\n- Regular working hours for uninterrupted three-shift work are defined as a maximum of eight hours per day and no more than 48 hours per week.\\n- The weekly working time is further clarified as a maximum of eight weekly days, which might refer to a specific scheduling arrangement where the total working hours for the week are distributed across eight days.\\n\\nIt appears that the context provided focuses more on the timing and conditions for implementing these work systems rather than detailing the specific provisions for each. The agreement seems to emphasize the importance of local negotiations and setting a clear timeline for the transition to uninterrupted two-shift work, while also establishing the regular working hours for uninterrupted three-shift work. For precise details on the provisions, it would be advisable to refer to the relevant local agreements or labor regulations that govern these work systems.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input what you want to ask so the chatbot will answer you about the questions \n",
    "# Ask In English \n",
    "generate_response(\"What are the provisions for uninterrupted two-shift and three-shift work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Keskeytymättömän Kaksivuorotyön (KKT) Määräykset:**\\n- Keskeytymättömän kaksivuorotyön ehdot ja siirtyminen keskeytymättömään kaksivuorotyöhön sovitaan paikallisesti sopimalla osapuolten välillä.\\n- Osapuolet sopivat, että uudet määräykset eivät vaikuta ennen 01.12.2021 yrityksessä jo käytössä oleviin keskeytymättömän kaksivuorotyöhön liittyviin työaikajärjestelmiin.\\n\\n**Keskeytymättömän Kolmivuorotyön (KKT) Määräykset:**\\n- Säännöllinen työaika keskeytymättömässä kolmivuorotyössä on enintään 8 tuntia vuorokaudessa ja 48 tuntia viikossa.\\n- Viikoittainen työaika on rajoitettu enintään kahdeksaan viikkoa pidemmälle.\\n\\nNäiden määräysten tarkoitus on säätää työaikaa ja varmistaa, että työntekijät saavat riittävän levon ja työskentelevät turvallisissa ehdoissa, erityisesti vaatimallisissa vuorotyöskenteissä. Soveltamisohjeet antavat työnantajille ja työntekijöille selkeyttä siitä, miten noudattaa näitä määräyksiä ja soveltaa niitä paikallisesti sopivalla tavalla.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask In Finnish \n",
    "generate_response(\"Mitkä ovat keskeytymättömän kaksi- ja kolmivuorotyön määräykset?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where We Can Improve : \n",
    "\n",
    "##### RAG-bot answers questions effectively, though it currently draws from only two documents. To boost its accuracy, we can expand the document base and test a variety of prompts, improving the bot’s performance and reliability. Additionally, adjusting the context window can yield more comprehensive answers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "recuritment_excercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
